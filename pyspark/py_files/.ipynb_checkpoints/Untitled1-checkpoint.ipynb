{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d6026b-bc2c-4be7-a691-052bbe6de430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, struct, udf,to_json,from_json\n",
    "from pyspark.ml.linalg import Vectors,DenseVector,SparseVector\n",
    "from pyspark.ml.feature import SQLTransformer,VectorAssembler\n",
    "import joblib as jb\n",
    "import numpy as np\n",
    "import xgboost\n",
    "import fastavro\n",
    "import json\n",
    "import io\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from pyspark.sql.avro.functions import from_avro,to_avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb9a7f75-0a68-4470-878c-d8129ef65f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-278e411c-73cf-4393-9211-a08ed60cc984;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.5.5 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.5 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.5 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.3.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.3.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      ":: resolution report :: resolve 2038ms :: artifacts dl 29ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.3.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.3.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.5.5 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.5 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.5 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   31  |   0   |   0   |   2   ||   29  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-278e411c-73cf-4393-9211-a08ed60cc984\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 29 already retrieved (0kB/19ms)\n",
      "25/04/05 12:37:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://020a52f2e8d8:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f919c3fdca0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-avro_2.12:3.5.5,\"\n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.5,\"\n",
    "            \"com.datastax.spark:spark-cassandra-connector_2.12:3.3.0\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"host.docker.internal\") \\\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83467ffc-d140-4df6-b6ea-b78e6480d0a9",
   "metadata": {},
   "source": [
    "# Importing trainnig features stats \"mode,mean,deviation\" and categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1c8a01-4f46-4c0c-8325-df620f9c9342",
   "metadata": {},
   "source": [
    "- **Credit data features stats**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5088c43e-a6ed-4887-af08-0f1a7cc43fd7",
   "metadata": {},
   "source": [
    "- mean and deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c2dbcb4-05f9-411d-bb69-348eaba34f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+------------------+\n",
      "| info|            amount|     oldbalanceOrg|    newbalanceOrig|\n",
      "+-----+------------------+------------------+------------------+\n",
      "| mean| 627408.4007645844|1097893.0927459989| 610851.3635214248|\n",
      "|scale|1658502.6666142726| 3132228.697963737|2628933.6624411293|\n",
      "+-----+------------------+------------------+------------------+\n",
      "\n",
      "root\n",
      " |-- info: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- oldbalanceOrg: double (nullable = true)\n",
      " |-- newbalanceOrig: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_credit_data_mean_deviation=spark.read.csv(\"../csv/credit_data_mean_scale.csv\",header=True,inferSchema=True)\n",
    "df_credit_data_mean_deviation.show()\n",
    "df_credit_data_mean_deviation.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea874a1-2b7b-4ea5-8244-c034fc46b177",
   "metadata": {},
   "source": [
    "- mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8704688f-4cef-46ee-8bb3-9384c42d38aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|info|    type|\n",
      "+----+--------+\n",
      "|mode|CASH_OUT|\n",
      "+----+--------+\n",
      "\n",
      "root\n",
      " |-- info: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_credit_data_mode=spark.read.csv(\"../csv/credit_data_mode.csv\",header=True,inferSchema=True)\n",
    "df_credit_data_mode.show()\n",
    "df_credit_data_mode.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23849ee9-cd84-461e-b1c7-44d7519ae500",
   "metadata": {},
   "source": [
    "- Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decde9b5-55cb-4301-92c7-10c04fa08248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': ['CASH_OUT', 'DEBIT', 'PAYMENT', 'TRANSFER']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../json/credit_data_categorical_values.json\",\"r\") as f:\n",
    "    credit_data_categorical_encoder_dict=json.load(f)\n",
    "credit_data_categorical_encoder_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4c701e-ee55-4a9a-9226-914965aaae11",
   "metadata": {},
   "source": [
    "- **Insurance Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b9e30-1bb5-428f-86f0-94740881e57c",
   "metadata": {},
   "source": [
    "- mean and deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19461cf9-0143-4bb5-8a98-df0618660d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-------------------+-------------------+------------------+-------------------+\n",
      "| info|     marital_status|witness_present_ind| high_education_ind|past_num_of_claims| address_change_ind|\n",
      "+-----+-------------------+-------------------+-------------------+------------------+-------------------+\n",
      "| mean| 0.7152631578947368|0.23467105263157895| 0.6966447368421053|0.5023684210526316| 0.5731578947368421|\n",
      "|scale|0.45128901255535303|0.42379305054279226|0.45970734981322536|0.9544189034241184|0.49461896692067236|\n",
      "+-----+-------------------+-------------------+-------------------+------------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- info: string (nullable = true)\n",
      " |-- marital_status: double (nullable = true)\n",
      " |-- witness_present_ind: double (nullable = true)\n",
      " |-- high_education_ind: double (nullable = true)\n",
      " |-- past_num_of_claims: double (nullable = true)\n",
      " |-- address_change_ind: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_insurance_data_mean_deviation=spark.read.csv(\"../csv/insurance_data_mean_scale.csv\",header=True,inferSchema=True)\n",
    "df_insurance_data_mean_deviation.show()\n",
    "df_insurance_data_mean_deviation.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14be7e2a-cac6-43e6-a3fe-c55cdca2bf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+-------------+-------------+----------------+\n",
      "|info|gender|channel|accident_site|living_status|vehicle_category|\n",
      "+----+------+-------+-------------+-------------+----------------+\n",
      "|mode|     M| Broker|        Local|          Own|         Compact|\n",
      "+----+------+-------+-------------+-------------+----------------+\n",
      "\n",
      "root\n",
      " |-- info: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- accident_site: string (nullable = true)\n",
      " |-- living_status: string (nullable = true)\n",
      " |-- vehicle_category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_insurance_data_mode=spark.read.csv(\"../csv/insurance_data_mode.csv\",header=True,inferSchema=True)\n",
    "df_insurance_data_mode.show()\n",
    "df_insurance_data_mode.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ccc403-2c35-479a-8a8a-6b34f1de50fd",
   "metadata": {},
   "source": [
    "- Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6018265-aff7-4d27-8e51-3874c9e55489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gender': ['M'],\n",
       " 'channel': ['Phone'],\n",
       " 'accident_site': ['Parking_Lot'],\n",
       " 'living_status': ['Rent'],\n",
       " 'vehicle_category': ['Medium']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../json/insurance_data_categorical_values.json\",\"r\") as f:\n",
    "    insurance_data_categorical_encoder_dict=json.load(f)\n",
    "insurance_data_categorical_encoder_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f32b07-89f4-4c1e-a33b-86c57a522667",
   "metadata": {},
   "source": [
    "# Loading the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1f3c930-b087-492f-bfce-7f276d201a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_model_path=\"../models/credit_fraud_detection_model.pkl\"\n",
    "insurance_model_path=\"../models/insurance_fraud_detection_model.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e437786-0c15-4214-b795-656005a2430b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.5, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=7, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=1, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBClassifier</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.5, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=7, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=1, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.5, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=7, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=1, ...)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(credit_model_path, \"rb\") as credit_model_file:\n",
    "    credit_model = jb.load(credit_model_file)\n",
    "credit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39436e11-d799-4ddf-b65e-986fae30ca75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=20, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=30, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=1, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBClassifier</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=20, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=30, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=1, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=20, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=30, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=1, ...)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(insurance_model_path,\"rb\") as insurance_model_file:\n",
    "    insurance_model=jb.load(insurance_model_file)\n",
    "insurance_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59213e83-ac2d-4ddf-8c0b-607be8fe5047",
   "metadata": {},
   "source": [
    "# Creating data pipelines and preprocessors  for each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de42e0b9-fdbb-4aa7-be2e-fdbdf43fd9ba",
   "metadata": {},
   "source": [
    "- Creating custom function to process and transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bbd1cb7-fccb-4656-874e-0d1b7ee720af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_infos_to_map(df_mean_deviation=None,df_mode=None):\n",
    "    if (df_mean_deviation==None) | (df_mode==None):\n",
    "        raise ValueError(\"Null dataFrames cant be accepted !\")\n",
    "    mean_scale_map={\n",
    "        col:{\n",
    "            'mean': df_mean_deviation.filter(df_mean_deviation['info'] == 'mean').select(col).collect()[0][0],\n",
    "            'deviation': df_mean_deviation.filter(df_mean_deviation['info'] == 'scale').select(col).collect()[0][0]\n",
    "        } for col in df_mean_deviation.columns[1:]\n",
    "    }\n",
    "    mode_map={\n",
    "        col : {\n",
    "            \"mode\":df_mode.filter(df_mode[\"info\"]=='mode').select(col).collect()[0][0],\n",
    "        } for col in df_mode.columns[1:] \n",
    "    }\n",
    "    featurs_stats={**mean_scale_map,**mode_map}\n",
    "    return featurs_stats\n",
    "\n",
    "\n",
    "def map_credit_features_infos(features_stats_dict=None,info=None,column=None):\n",
    "    valid_columns=list(features_stats_dict.keys())\n",
    "    valid_infos=['mean', 'deviation',\"mode\"]\n",
    "    if column not in valid_columns:\n",
    "        raise ValueError(f\"Invalid column name. Expected one of {valid_columns}, got '{column}'\")\n",
    "    if info not in valid_infos:\n",
    "        raise ValueError(f\"Invalid info name. Expected one of {valid_infos}, got '{info}'\")\n",
    "    if info not in features_stats_dict[column].keys():\n",
    "        raise ValueError(f\"Invalid info name. Expected one of {list(features_stats_dict[column].keys())}, got '{info}'\")\n",
    "    return features_stats_dict[column][info]\n",
    "\n",
    "# Custom inputer function\n",
    "def custom_imputer(num_cols=[],cat_cols=[],features_stats_dict=None):\n",
    "    imputer_sql_expressions=[\n",
    "        f\"COALESCE({col},{map_credit_features_infos(column=col,info='mean',features_stats_dict=features_stats_dict)}) as imputed_{col}\" \n",
    "        for col in num_cols\n",
    "    ]+[\n",
    "        f\"COALESCE({col},'{map_credit_features_infos(column=col,info='mode',features_stats_dict=features_stats_dict)}') as imputed_{col}\" \n",
    "        for col in cat_cols\n",
    "    ]\n",
    "    imputer_sql_expression=f\"SELECT *,{','.join(imputer_sql_expressions)} FROM __THIS__\"\n",
    "    print(imputer_sql_expression)\n",
    "    return SQLTransformer(statement=imputer_sql_expression)\n",
    "\n",
    "# custom scaler\n",
    "def custom_scaler(scale_cols=None,features_stats_dict=None):\n",
    "    scaler_sql_expressions=[\n",
    "        f\"(imputed_{col}-{map_credit_features_infos(column=col,info='mean',features_stats_dict=features_stats_dict)})/{map_credit_features_infos(column=col,info='deviation',features_stats_dict=features_stats_dict)} as scaled_{col}\"\n",
    "        for col in scale_cols\n",
    "    ]\n",
    "    scaler_sql_expression=f\"SELECT *,{','.join(scaler_sql_expressions)} FROM __THIS__\"\n",
    "    print(scaler_sql_expression)\n",
    "    return SQLTransformer(statement=scaler_sql_expression)\n",
    "\n",
    "# custom hot encoder\n",
    "def custom_hot_encoder(encoder_categories_dict=None):\n",
    "    encoders=[]\n",
    "    for col,categories in encoder_categories_dict.items():\n",
    "        encoder_sql_expressions=[\n",
    "            f\"CASE WHEN imputed_{col}='{cat}' THEN 1 ELSE 0 END as {col}_{cat}\"\n",
    "            for cat in categories\n",
    "        ]\n",
    "        encoder_sql_expression=f\"SELECT *,{','.join(encoder_sql_expressions)} FROM __THIS__\"\n",
    "        print(encoder_sql_expression)\n",
    "        encoder=SQLTransformer(statement=encoder_sql_expression)\n",
    "        encoders.append(encoder)\n",
    "    return encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a6b6d-de55-46e4-adc3-014e8392f5aa",
   "metadata": {},
   "source": [
    "- Creating data pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d15ac3a-6ea1-472b-80b0-d3f18dd3c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_pipline(df=None,df_mean_deviation=None,df_mode=None,encoder_categories_dict=None,output_features=None):\n",
    "    \n",
    "    features_stats_dict=features_infos_to_map(\n",
    "        df_mode=df_mode,\n",
    "        df_mean_deviation=df_mean_deviation\n",
    "    )\n",
    "    \n",
    "    # Filling missing values \n",
    "    num_cols=df_mean_deviation.columns[1:]\n",
    "    cat_cols=df_mode.columns[1:]\n",
    "    imputer=custom_imputer(\n",
    "        num_cols=num_cols,\n",
    "        cat_cols=cat_cols,\n",
    "        features_stats_dict=features_stats_dict\n",
    "    )\n",
    "    \n",
    "    # Scaling data\n",
    "    scale_cols=df_mean_deviation.columns[1:]\n",
    "    scaler=custom_scaler(\n",
    "        scale_cols=scale_cols,\n",
    "        features_stats_dict=features_stats_dict\n",
    "    )\n",
    "    \n",
    "    # Encoding categorical values\n",
    "    cat_encoders=custom_hot_encoder(\n",
    "        encoder_categories_dict=encoder_categories_dict\n",
    "    )\n",
    "\n",
    "    # Assembling all features in one vectore\n",
    "    assembler=VectorAssembler(\n",
    "        inputCols=output_features,\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "    # Creating the pipeline\n",
    "    pipline=Pipeline(stages=[imputer]+[scaler]+cat_encoders+[assembler])\n",
    "    return pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2554460-de20-4f84-b0b7-79f177500494",
   "metadata": {},
   "source": [
    "- Testing the pipline on credit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb57a386-4ef3-4098-976d-afaeb65bd485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Raw Credit Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------------+--------------+\n",
      "|    type| amount|oldbalanceOrg|newbalanceOrig|\n",
      "+--------+-------+-------------+--------------+\n",
      "|   DEBIT|  100.0|       5000.0|        4000.0|\n",
      "|TRANSFER| 2000.0|       6000.0|        4000.0|\n",
      "|CASH_OUT|70000.0|       2000.0|        1500.0|\n",
      "|TRANSFER|   NULL|       7000.0|        5000.0|\n",
      "|    NULL| 1500.0|         NULL|        3000.0|\n",
      "+--------+-------+-------------+--------------+\n",
      "\n",
      "SELECT *,COALESCE(amount,627408.4007645844) as imputed_amount,COALESCE(oldbalanceOrg,1097893.0927459989) as imputed_oldbalanceOrg,COALESCE(newbalanceOrig,610851.3635214248) as imputed_newbalanceOrig,COALESCE(type,'CASH_OUT') as imputed_type FROM __THIS__\n",
      "SELECT *,(imputed_amount-627408.4007645844)/1658502.6666142726 as scaled_amount,(imputed_oldbalanceOrg-1097893.0927459989)/3132228.697963737 as scaled_oldbalanceOrg,(imputed_newbalanceOrig-610851.3635214248)/2628933.6624411293 as scaled_newbalanceOrig FROM __THIS__\n",
      "SELECT *,CASE WHEN imputed_type='CASH_OUT' THEN 1 ELSE 0 END as type_CASH_OUT,CASE WHEN imputed_type='DEBIT' THEN 1 ELSE 0 END as type_DEBIT,CASE WHEN imputed_type='PAYMENT' THEN 1 ELSE 0 END as type_PAYMENT,CASE WHEN imputed_type='TRANSFER' THEN 1 ELSE 0 END as type_TRANSFER FROM __THIS__\n",
      "--------------Transformed Credit Data\n",
      "+--------+-----------------+---------------------+----------------------+------------+--------------------+--------------------+---------------------+-------------+----------+------------+-------------+--------------------+\n",
      "|    type|   imputed_amount|imputed_oldbalanceOrg|imputed_newbalanceOrig|imputed_type|       scaled_amount|scaled_oldbalanceOrg|scaled_newbalanceOrig|type_CASH_OUT|type_DEBIT|type_PAYMENT|type_TRANSFER|            features|\n",
      "+--------+-----------------+---------------------+----------------------+------------+--------------------+--------------------+---------------------+-------------+----------+------------+-------------+--------------------+\n",
      "|   DEBIT|            100.0|               5000.0|                4000.0|       DEBIT|-0.37823780051267236|-0.34891867680558863| -0.23083555594853822|            0|         1|           0|            0|[-0.3782378005126...|\n",
      "|TRANSFER|           2000.0|               6000.0|                4000.0|    TRANSFER| -0.3770921888484604| -0.3485994153159503| -0.23083555594853822|            0|         0|           0|            1|[-0.3770921888484...|\n",
      "|CASH_OUT|          70000.0|               2000.0|                1500.0|    CASH_OUT|-0.33609135033982074| -0.3498764612745038| -0.23178651185728436|            1|         0|           0|            0|[-0.3360913503398...|\n",
      "|TRANSFER|627408.4007645844|               7000.0|                5000.0|    TRANSFER|                 0.0| -0.3482801538263119| -0.23045517358503978|            0|         0|           0|            1|(7,[1,2,6],[-0.34...|\n",
      "|    NULL|           1500.0|   1097893.0927459989|                3000.0|    CASH_OUT| -0.3773936656022004|                 0.0| -0.23121593831203668|            1|         0|           0|            0|(7,[0,2,3],[-0.37...|\n",
      "+--------+-----------------+---------------------+----------------------+------------+--------------------+--------------------+---------------------+-------------+----------+------------+-------------+--------------------+\n",
      "\n",
      "--------------Credit Data features'Vectors'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([-0.3782, -0.3489, -0.2308, 0.0, 1.0, 0.0, 0.0])),\n",
       " Row(features=DenseVector([-0.3771, -0.3486, -0.2308, 0.0, 0.0, 0.0, 1.0])),\n",
       " Row(features=DenseVector([-0.3361, -0.3499, -0.2318, 1.0, 0.0, 0.0, 0.0])),\n",
       " Row(features=SparseVector(7, {1: -0.3483, 2: -0.2305, 6: 1.0})),\n",
       " Row(features=SparseVector(7, {0: -0.3774, 2: -0.2312, 3: 1.0}))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_data = [\n",
    "    (\"DEBIT\",100.,5000.,4000.),\n",
    "    (\"TRANSFER\",2000.,6000.,4000.),\n",
    "    (\"CASH_OUT\",70000.,2000.,1500.),\n",
    "    (\"TRANSFER\",None,7000.,5000.), \n",
    "    (None,1500.,None,3000.) \n",
    "]\n",
    "\n",
    "credit_data_schema = StructType([\n",
    "    StructField(\"type\",StringType(),True),\n",
    "    StructField(\"amount\",DoubleType(),True),\n",
    "    StructField(\"oldbalanceOrg\",DoubleType(),True),\n",
    "    StructField(\"newbalanceOrig\",DoubleType(),True),\n",
    "])\n",
    "\n",
    "df_test_credit_data=spark.createDataFrame(\n",
    "    credit_data, \n",
    "    schema=credit_data_schema\n",
    ")\n",
    "print(\"--------------Raw Credit Data\")\n",
    "df_test_credit_data.show()\n",
    "\n",
    "\n",
    "credit_data_output_features=[\"scaled_amount\",\"scaled_oldbalanceOrg\",\"scaled_newbalanceOrig\",\"type_CASH_OUT\",\"type_DEBIT\",\"type_PAYMENT\",\"type_TRANSFER\"]\n",
    "\n",
    "credit_data_pipline=create_data_pipline(\n",
    "    df=df_test_credit_data,\n",
    "    df_mean_deviation=df_credit_data_mean_deviation,\n",
    "    df_mode=df_credit_data_mode,\n",
    "    encoder_categories_dict=credit_data_categorical_encoder_dict,\n",
    "    output_features=credit_data_output_features\n",
    ")\n",
    "\n",
    "transformed_credit_data=credit_data_pipline.fit(df_test_credit_data).transform(df_test_credit_data)\n",
    "print(\"--------------Transformed Credit Data\")\n",
    "transformed_credit_data.drop(\"amount\",\"oldbalanceOrg\",\"newbalanceOrig\").show()\n",
    "\n",
    "print(\"--------------Credit Data features'Vectors'\")\n",
    "transformed_credit_data.select(\"features\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ecd5590-b462-4015-817c-f9a2e0108f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Raw insurance Data\n",
      "+------------------+------------------+------+------------------+-------------------+--------------+-------+-------------+-------------+----------------+\n",
      "|high_education_ind|past_num_of_claims|gender|address_change_ind|witness_present_ind|marital_status|channel|accident_site|living_status|vehicle_category|\n",
      "+------------------+------------------+------+------------------+-------------------+--------------+-------+-------------+-------------+----------------+\n",
      "|              NULL|                 2|     M|                 0|                  1|             1| Broker|         NULL|          Own|         Compact|\n",
      "|                 1|                 0|     F|                 1|                  0|             0|   NULL|        Local|         Rent|            NULL|\n",
      "|                 0|                 1|     M|                 0|                  1|             1| Online|  Parking Lot|          Own|           Large|\n",
      "|                 1|                 0|     F|                 1|                  0|             0|  Phone|        Local|         Rent|          Medium|\n",
      "|                 0|              NULL|     M|                 1|                  1|             1| Broker|      Highway|         NULL|         Compact|\n",
      "|                 1|                 1|     F|              NULL|                  0|             0| Online|  Parking Lot|          Own|          Medium|\n",
      "|                 0|                 2|     M|                 1|               NULL|             1|   NULL|        Local|         Rent|           Large|\n",
      "|                 1|                 0|     F|                 0|                  0|          NULL| Broker|      Highway|          Own|         Compact|\n",
      "+------------------+------------------+------+------------------+-------------------+--------------+-------+-------------+-------------+----------------+\n",
      "\n",
      "--------------Transformed insurance Data\n",
      "SELECT *,COALESCE(marital_status,0.7152631578947368) as imputed_marital_status,COALESCE(witness_present_ind,0.23467105263157895) as imputed_witness_present_ind,COALESCE(high_education_ind,0.6966447368421053) as imputed_high_education_ind,COALESCE(past_num_of_claims,0.5023684210526316) as imputed_past_num_of_claims,COALESCE(address_change_ind,0.5731578947368421) as imputed_address_change_ind,COALESCE(gender,'M') as imputed_gender,COALESCE(channel,'Broker') as imputed_channel,COALESCE(accident_site,'Local') as imputed_accident_site,COALESCE(living_status,'Own') as imputed_living_status,COALESCE(vehicle_category,'Compact') as imputed_vehicle_category FROM __THIS__\n",
      "SELECT *,(imputed_marital_status-0.7152631578947368)/0.45128901255535303 as scaled_marital_status,(imputed_witness_present_ind-0.23467105263157895)/0.42379305054279226 as scaled_witness_present_ind,(imputed_high_education_ind-0.6966447368421053)/0.45970734981322536 as scaled_high_education_ind,(imputed_past_num_of_claims-0.5023684210526316)/0.9544189034241184 as scaled_past_num_of_claims,(imputed_address_change_ind-0.5731578947368421)/0.49461896692067236 as scaled_address_change_ind FROM __THIS__\n",
      "SELECT *,CASE WHEN imputed_gender='M' THEN 1 ELSE 0 END as gender_M FROM __THIS__\n",
      "SELECT *,CASE WHEN imputed_channel='Phone' THEN 1 ELSE 0 END as channel_Phone FROM __THIS__\n",
      "SELECT *,CASE WHEN imputed_accident_site='Parking_Lot' THEN 1 ELSE 0 END as accident_site_Parking_Lot FROM __THIS__\n",
      "SELECT *,CASE WHEN imputed_living_status='Rent' THEN 1 ELSE 0 END as living_status_Rent FROM __THIS__\n",
      "SELECT *,CASE WHEN imputed_vehicle_category='Medium' THEN 1 ELSE 0 END as vehicle_category_Medium FROM __THIS__\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 12:37:52 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Insurance Data features'Vectors'\n",
      "+----------------------+---------------------------+--------------------------+--------------------------+--------------------------+--------------+---------------+---------------------+---------------------+------------------------+---------------------+--------------------------+-------------------------+-------------------------+-------------------------+--------+-------------+-------------------------+------------------+-----------------------+\n",
      "|imputed_marital_status|imputed_witness_present_ind|imputed_high_education_ind|imputed_past_num_of_claims|imputed_address_change_ind|imputed_gender|imputed_channel|imputed_accident_site|imputed_living_status|imputed_vehicle_category|scaled_marital_status|scaled_witness_present_ind|scaled_high_education_ind|scaled_past_num_of_claims|scaled_address_change_ind|gender_M|channel_Phone|accident_site_Parking_Lot|living_status_Rent|vehicle_category_Medium|\n",
      "+----------------------+---------------------------+--------------------------+--------------------------+--------------------------+--------------+---------------+---------------------+---------------------+------------------------+---------------------+--------------------------+-------------------------+-------------------------+-------------------------+--------+-------------+-------------------------+------------------+-----------------------+\n",
      "|    1.0000000000000000|        1.00000000000000000|        0.6966447368421053|        2.0000000000000000|        0.0000000000000000|             M|         Broker|                Local|                  Own|                 Compact|         0.6309412243|              1.8059025423|             0.0000000000|            1.56915540291|            -1.1587867289|       1|            0|                        0|                 0|                      0|\n",
      "|    0.0000000000000000|        0.00000000000000000|        1.0000000000000000|        0.0000000000000000|        1.0000000000000000|             F|         Broker|                Local|                 Rent|                 Compact|        -1.5849336855|             -0.5537397377|             0.6598877814|           -0.52636051031|             0.8629715676|       0|            0|                        0|                 1|                      0|\n",
      "|    1.0000000000000000|        1.00000000000000000|        0.0000000000000000|        1.0000000000000000|        0.0000000000000000|             M|         Online|          Parking Lot|                  Own|                   Large|         0.6309412243|              1.8059025423|            -1.5154091774|            0.52139744630|            -1.1587867289|       1|            0|                        0|                 0|                      0|\n",
      "|    0.0000000000000000|        0.00000000000000000|        1.0000000000000000|        0.0000000000000000|        1.0000000000000000|             F|          Phone|                Local|                 Rent|                  Medium|        -1.5849336855|             -0.5537397377|             0.6598877814|           -0.52636051031|             0.8629715676|       0|            1|                        0|                 1|                      1|\n",
      "|    1.0000000000000000|        1.00000000000000000|        0.0000000000000000|        0.5023684210526316|        1.0000000000000000|             M|         Broker|              Highway|                  Own|                 Compact|         0.6309412243|              1.8059025423|            -1.5154091774|            0.00000000000|             0.8629715676|       1|            0|                        0|                 0|                      0|\n",
      "|    0.0000000000000000|        0.00000000000000000|        1.0000000000000000|        1.0000000000000000|        0.5731578947368421|             F|         Online|          Parking Lot|                  Own|                  Medium|        -1.5849336855|             -0.5537397377|             0.6598877814|            0.52139744630|             0.0000000000|       0|            0|                        0|                 0|                      1|\n",
      "|    1.0000000000000000|        0.23467105263157895|        0.0000000000000000|        2.0000000000000000|        1.0000000000000000|             M|         Broker|                Local|                 Rent|                   Large|         0.6309412243|              0.0000000000|            -1.5154091774|            1.56915540291|             0.8629715676|       1|            0|                        0|                 1|                      0|\n",
      "|    0.7152631578947368|        0.00000000000000000|        1.0000000000000000|        0.0000000000000000|        0.0000000000000000|             F|         Broker|              Highway|                  Own|                 Compact|         0.0000000000|             -0.5537397377|             0.6598877814|           -0.52636051031|            -1.1587867289|       0|            0|                        0|                 0|                      0|\n",
      "+----------------------+---------------------------+--------------------------+--------------------------+--------------------------+--------------+---------------+---------------------+---------------------+------------------------+---------------------+--------------------------+-------------------------+-------------------------+-------------------------+--------+-------------+-------------------------+------------------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(10, {1: 1.5692, 2: 1.0, 3: -1.1588, 4: 1.8059, 5: 0.6309})),\n",
       " Row(features=DenseVector([0.6599, -0.5264, 0.0, 0.863, -0.5537, -1.5849, 0.0, 0.0, 1.0, 0.0])),\n",
       " Row(features=DenseVector([-1.5154, 0.5214, 1.0, -1.1588, 1.8059, 0.6309, 0.0, 0.0, 0.0, 0.0])),\n",
       " Row(features=DenseVector([0.6599, -0.5264, 0.0, 0.863, -0.5537, -1.5849, 1.0, 0.0, 1.0, 1.0])),\n",
       " Row(features=SparseVector(10, {0: -1.5154, 2: 1.0, 3: 0.863, 4: 1.8059, 5: 0.6309})),\n",
       " Row(features=SparseVector(10, {0: 0.6599, 1: 0.5214, 4: -0.5537, 5: -1.5849, 9: 1.0})),\n",
       " Row(features=DenseVector([-1.5154, 1.5692, 1.0, 0.863, 0.0, 0.6309, 0.0, 0.0, 1.0, 0.0])),\n",
       " Row(features=SparseVector(10, {0: 0.6599, 1: -0.5264, 3: -1.1588, 4: -0.5537}))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insurance_schema=StructType(\n",
    "    [\n",
    "        StructField('high_education_ind',IntegerType(),True),\n",
    "        StructField('past_num_of_claims',IntegerType(),True),\n",
    "        StructField('gender',StringType(),True),\n",
    "        StructField('address_change_ind',IntegerType(),True),\n",
    "        StructField('witness_present_ind',IntegerType(),True),\n",
    "        StructField('marital_status',IntegerType(),True),\n",
    "        StructField('channel',StringType(),True),\n",
    "        StructField('accident_site',StringType(),True),\n",
    "        StructField('living_status',StringType(),True),\n",
    "        StructField('vehicle_category',StringType(),True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "insurance_data = [\n",
    "    (None, 2, \"M\", 0, 1, 1, \"Broker\", None, \"Own\", \"Compact\"),\n",
    "    (1, 0, \"F\", 1, 0, 0, None, \"Local\", \"Rent\", None),\n",
    "    ( 0, 1, \"M\", 0, 1, 1, \"Online\", \"Parking Lot\", \"Own\", \"Large\"),\n",
    "    ( 1, 0, \"F\", 1, 0, 0, \"Phone\", \"Local\", \"Rent\", \"Medium\"),\n",
    "    (0, None, \"M\", 1, 1, 1, \"Broker\", \"Highway\", None, \"Compact\"),\n",
    "    ( 1, 1, \"F\", None, 0, 0, \"Online\", \"Parking Lot\", \"Own\", \"Medium\"),\n",
    "    (0, 2, \"M\", 1, None, 1, None, \"Local\", \"Rent\", \"Large\"),\n",
    "    ( 1, 0, \"F\", 0, 0, None, \"Broker\", \"Highway\", \"Own\", \"Compact\"),\n",
    "]\n",
    "\n",
    "print(\"--------------Raw insurance Data\")\n",
    "df_insurance_test = spark.createDataFrame(insurance_data, insurance_schema)\n",
    "df_insurance_test.show()\n",
    "\n",
    "output_features=[\"scaled_high_education_ind\",\"scaled_past_num_of_claims\",\"gender_M\",\"scaled_address_change_ind\",\n",
    "                   \"scaled_witness_present_ind\",\"scaled_marital_status\",\"channel_Phone\",\"accident_site_Parking_Lot\",\"living_status_Rent\",\n",
    "                   \"vehicle_category_Medium\"\n",
    "                  ]\n",
    "print(\"--------------Transformed insurance Data\")\n",
    "insurance_data_pipeline=create_data_pipline(\n",
    "    df=df_insurance_test, \n",
    "    df_mean_deviation=df_insurance_data_mean_deviation,\n",
    "    df_mode=df_insurance_data_mode,\n",
    "    encoder_categories_dict=insurance_data_categorical_encoder_dict,\n",
    "    output_features=output_features\n",
    ")\n",
    "\n",
    "insurance_pipeline_model=insurance_data_pipeline.fit(df_insurance_test)\n",
    "transformed_insurance_df=insurance_pipeline_model.transform(df_insurance_test)\n",
    "\n",
    "print(\"--------------Insurance Data features'Vectors'\")\n",
    "transformed_insurance_df.drop(\n",
    "    \"high_education_ind\",\n",
    "    \"past_num_of_claims\",\n",
    "    \"gender\",\n",
    "    \"address_change_ind\",\n",
    "    \"witness_present_ind\",\n",
    "    \"marital_status\",\n",
    "    \"channel\",\n",
    "    \"accident_site\",\n",
    "    \"living_status\",\n",
    "    \"vehicle_category\",\n",
    "    \"features\"\n",
    ").show()\n",
    "transformed_insurance_df.select(\"features\").collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93c91d2-1545-4c3c-8c96-733a2db5d203",
   "metadata": {},
   "source": [
    "# Kafka Confuguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98c4f7aa-b87b-44a6-8a95-ac248336ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_broker=\"host.docker.internal:9092\"\n",
    "shcema_registry=\"http://host.docker.internal:8081\"\n",
    "insurance_input_topic=\"raw_insurance_data\"\n",
    "insurance_output_topic=\"processed_insurance\"\n",
    "credit_input_topic=\"raw_credit_data\"\n",
    "credit_output_topic=\"processed_credit_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeb4e9d-08ce-43ac-9cb3-2e099ec43afd",
   "metadata": {},
   "source": [
    "# Defining shcemas for kafka topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40085bce-3248-424f-ae89-4bc4f2373220",
   "metadata": {},
   "source": [
    "- Raw credit data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2e403de-012f-4328-8f84-7d2ecb3d5bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_credit_data_schema=StructType(\n",
    "    [\n",
    "        StructField('client_id',StringType(),False),\n",
    "        StructField('transaction_id',StringType(),False),\n",
    "        StructField(\"type\",StringType(),True),\n",
    "        StructField(\"amount\",DoubleType(),True),\n",
    "        StructField(\"oldbalanceOrg\",DoubleType(),True),\n",
    "        StructField(\"newbalanceOrig\",DoubleType(),True), \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff3eb71-6582-4fc6-9fcc-65982d2f021f",
   "metadata": {},
   "source": [
    "- Raw insurance data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e4a8f2f-c510-46a1-83df-478f1c7cbac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_insurance_data_schema=StructType(\n",
    "    [\n",
    "        StructField('client_id',StringType(),False),\n",
    "        StructField('transaction_id',StringType(),False),\n",
    "        StructField('high_education_ind',IntegerType(),True),\n",
    "        StructField('past_num_of_claims',IntegerType(),True),\n",
    "        StructField('gender',StringType(),True),\n",
    "        StructField('address_change_ind',IntegerType(),True),\n",
    "        StructField('witness_present_ind',IntegerType(),True),\n",
    "        StructField('marital_status',IntegerType(),True),\n",
    "        StructField('channel',StringType(),True),\n",
    "        StructField('accident_site',StringType(),True),\n",
    "        StructField('living_status',StringType(),True),\n",
    "        StructField('vehicle_category',StringType(),True),    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a85b685-16db-4b4e-a38e-6d3c91d80632",
   "metadata": {},
   "source": [
    "# Stream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b567d74f-a4b0-4a5f-bf73-1e5b4e764527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that fetch schema from schema regitry if it's working ,if not it load it the last used one from ../schemas \n",
    "def fetch_schema(schema_regitry_url=None,topic=None):\n",
    "    schema_registry_client = SchemaRegistryClient({\"url\": schema_regitry_url})\n",
    "    try:\n",
    "        latest_schema=schema_registry_client.get_latest_version(f\"{topic}-value\")\n",
    "        schema_str=latest_schema.schema.schema_str\n",
    "        \n",
    "        schema_json=json.loads(schema_str)\n",
    "        with open(f\"../schemas/hard_coded_{topic}_schema.avsc\",\"w\") as f:\n",
    "            json.dump(schema_json,f)\n",
    "        print(f\"Successfully fetched and stored schema for {topic}-value\")\n",
    "        return schema_json\n",
    "    except Exception as e:\n",
    "        with open(f\"../schemas/hard_coded_{topic}_schema.avsc\",\"r\") as f:\n",
    "            schema_json=json.load(f)\n",
    "        return schema_json\n",
    "\n",
    "# Function to deserialize Avro data using fastavro\n",
    "def deserialize_avro(avro_bytes, schema):\n",
    "    try:\n",
    "        bytes_io = io.BytesIO(avro_bytes)\n",
    "        reader = fastavro.reader(bytes_io, schema)\n",
    "        for record in reader:\n",
    "            return record  \n",
    "    except Exception as e:\n",
    "        print(f\"Error deserializing Avro message: {e}\")\n",
    "        return None\n",
    "\n",
    "# Register UDF for deserialization\n",
    "def create_udf_for_deserialization(avro_schema):\n",
    "    def deserialize_udf(avro_bytes):\n",
    "        if avro_bytes is not None:\n",
    "            record = deserialize_avro(avro_bytes, avro_schema)\n",
    "            return json.dumps(record) if record else None\n",
    "        return None\n",
    "    return udf(deserialize_udf,StringType())\n",
    "\n",
    "def create_udf_for_prediction_serialization(avro_schema):\n",
    "    def serialize_avro(fraud):\n",
    "        row_dict = {\"prediction\": fraud}\n",
    "        buff = io.BytesIO()\n",
    "        fastavro.writer(buff, avro_schema, [row_dict])\n",
    "        return buff.getvalue()  \n",
    "    return udf(serialize_avro, BinaryType())  \n",
    "\n",
    "def create_udf_for_prediction(ml_model):\n",
    "    def predict(vector):\n",
    "        if isinstance(vector,DenseVector):\n",
    "            vector= np.array(vector)\n",
    "        elif isinstance(vector,SparseVector):\n",
    "            vector= np.array(vector.toArray())\n",
    "        elif isinstance(vector,list) or isinstance(vector,np.ndarray):\n",
    "            vector= np.array(vector)\n",
    "        else :\n",
    "            raise ValueError(f\"Expected one of these [np.ndarray, list], but got {type(vector)}\")\n",
    "\n",
    "        if vector.ndim == 1:\n",
    "            vector =vector.reshape(1, -1)\n",
    "            \n",
    "        return int(ml_model.predict(vector))\n",
    "    return udf(predict,IntegerType())\n",
    "    \n",
    "# Function that stream data from a kafka topic\n",
    "def read_kafka_stream(broker, topic):\n",
    "    return spark.readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", broker) \\\n",
    "        .option(\"subscribe\", topic) \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"failOnDataLoss\", \"false\") \\\n",
    "        .load()\n",
    "    \n",
    "def write_to_cassandra(df_batch, batch_id,cassandra_infos):\n",
    "    df_batch = df_batch.toDF(*[col.lower() for col in df_batch.columns])\n",
    "    df_batch.write \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .option(\"keyspace\", cassandra_infos[\"keyspace\"]) \\\n",
    "        .option(\"table\", cassandra_infos[\"table\"]) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "\n",
    "# Function that process stream \n",
    "def process_stream(kafka_infos, cassandra_infos,pipeline, ml_model, pyspark_schema):\n",
    "    # Extract only the value field from the raw kafka stream and cast it as a binary\n",
    "    raw_df = read_kafka_stream(kafka_infos[\"input\"][\"broker\"], kafka_infos[\"input\"][\"topic\"])\\\n",
    "        .selectExpr(\"CAST(value AS BINARY) as value\")\n",
    "\n",
    "    # Deserialize Avro using Fastavro\n",
    "    deserialize_avro_udf=create_udf_for_deserialization(kafka_infos[\"input\"][\"schema\"])\n",
    "    deserialized_df = raw_df.withColumn(\"deserialized_json\", deserialize_avro_udf(raw_df.value))\n",
    "\n",
    "    # Structring deserilized data \"json-format\" into a pyspark dataframe\n",
    "    structured_df = deserialized_df\\\n",
    "        .select(from_json(col(\"deserialized_json\"), pyspark_schema).alias(\"data\"))\\\n",
    "        .select(\"data.*\")\n",
    "    \n",
    "    cassandra_output_cols=structured_df.columns+[\"fraud\"]\n",
    "\n",
    "    # transforming data frame feateares \"scaling,cat encoding...\" and assembling them into a vector\n",
    "    transformed_df=pipeline.fit(structured_df).transform(structured_df)\n",
    "\n",
    "    # Predicting Fraud statue\n",
    "    predition_udf=create_udf_for_prediction(ml_model)\n",
    "    prediction_df=transformed_df\\\n",
    "        .withColumn(\"fraud\",predition_udf(transformed_df.features))\n",
    "\n",
    "    # df that will be written to cassandra\n",
    "    cassandra_output_df=prediction_df.select(cassandra_output_cols)\n",
    "\n",
    "    # df that will be passed to kafka so the client can consume it\n",
    "    serialize_avro_udf=create_udf_for_prediction_serialization(kafka_infos[\"output\"][\"schema\"])\n",
    "    kafka_output_df=prediction_df\\\n",
    "        .withColumnRenamed(\"transaction_id\",\"key\")\\\n",
    "        .withColumn(\"value\",serialize_avro_udf(prediction_df.fraud))\n",
    "\n",
    "    kafka_output_query=kafka_output_df.select([\"key\",\"value\"])\\\n",
    "        .writeStream\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .format(\"kafka\")\\\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_infos[\"output\"][\"broker\"]) \\\n",
    "        .option(\"topic\", kafka_infos[\"output\"][\"topic\"]) \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/kafka_checkpoint\") \\\n",
    "        .start()\n",
    "    \n",
    "    cassandra_output_query = cassandra_output_df.select(cassandra_output_cols)\\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .foreachBatch(lambda df_batch, batch_id:write_to_cassandra(df_batch, batch_id,cassandra_infos)) \\\n",
    "        #.trigger(processingTime=\"1 minute\")\\\n",
    "        .start()\n",
    "\n",
    "    # Debugging\n",
    "    #console_output_query = cassandra_output_df.select(cassandra_output_cols)\\\n",
    "     #   .writeStream \\\n",
    "      #  .outputMode(\"append\") \\\n",
    "       # .format(\"console\") \\\n",
    "        # .option(\"truncate\", False) \\\n",
    "         # .start()\n",
    "\n",
    "    return kafka_output_query,cassandra_output_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd5d1b1-1396-4525-832a-0f9bca4f737c",
   "metadata": {},
   "source": [
    "# SHIT SHOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f03cdb-59b8-401f-9d5d-2188530c0b8d",
   "metadata": {},
   "source": [
    "- Testing the process_stream function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "352d25c6-dde3-448d-b4fb-ada6a994e07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 12:37:55 WARN ControlConnection: [s0] Error connecting to Node(endPoint=host.docker.internal/fdc4:f303:9324:0:0:0:0:254:9042, hostId=null, hashCode=6c7113f2), trying next node (AnnotatedSocketException: Network is unreachable: host.docker.internal/fdc4:f303:9324:0:0:0:0:254:9042)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connexion réussie ! Voici la liste des keyspaces :\n",
      "+---------+--------------+------+-----+--------------+-------------+----+\n",
      "|client_id|transaction_id|amount|fraud|newbalanceorig|oldbalanceorg|type|\n",
      "+---------+--------------+------+-----+--------------+-------------+----+\n",
      "+---------+--------------+------+-----+--------------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = spark.read \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .options(table=\"credit_data\", keyspace=\"fraud_detection\") \\\n",
    "        .load()\n",
    "    \n",
    "    print(\"Connexion réussie ! Voici la liste des keyspaces :\")\n",
    "    df.show()\n",
    "except Exception as e:\n",
    "    print(\"Erreur de connexion à Cassandra :\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d472929-d10e-4b7d-8760-024dff0765b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched and stored schema for fraud_prediction-value\n",
      "Successfully fetched and stored schema for raw_credit_data-value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 12:38:03 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/04/05 12:38:03 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-461584bd-957f-4506-90d2-5821be04267b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/04/05 12:38:03 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/04/05 12:38:03 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/04/05 12:38:03 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/04/05 12:43:24 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "/tmp/ipykernel_872/2343669047.py:60: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:43:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "/tmp/ipykernel_872/2343669047.py:60: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "25/04/05 12:44:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:44:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "/tmp/ipykernel_872/2343669047.py:60: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "25/04/05 12:47:59 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:47:59 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:47:59 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:47:59 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:47:59 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:47:59 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:47:59 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:47:59 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:47:59 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:48:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:48:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:48:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:48:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:48:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:48:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:48:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:48:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:48:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:48:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "/tmp/ipykernel_872/2343669047.py:60: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "/tmp/ipykernel_872/2343669047.py:60: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "25/04/05 12:48:04 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "/tmp/ipykernel_872/2343669047.py:60: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "25/04/05 12:48:46 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:48:46 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:48:46 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "/tmp/ipykernel_872/2343669047.py:60: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "25/04/05 12:48:54 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:48:54 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:48:54 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:48:54 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:48:54 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:48:54 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:48:54 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "/tmp/ipykernel_872/2343669047.py:60: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "25/04/05 12:49:10 ERROR CassandraConnectorConf: Unknown host 'host.docker.internal'\n",
      "java.net.UnknownHostException: host.docker.internal: Temporary failure in name resolution\n",
      "\tat java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:929)\n",
      "\tat java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1529)\n",
      "\tat java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:848)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1519)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1378)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)\n",
      "\tat java.base/java.net.InetAddress.getByName(InetAddress.java:1256)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnectorConf$.maybeResolveHostAndPort(CassandraConnectorConf.scala:346)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnectorConf$.$anonfun$getIpBasedContactInfoFromSparkConf$1(CassandraConnectorConf.scala:379)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n",
      "\tat scala.collection.immutable.Set$Set1.foreach(Set.scala:141)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnectorConf$.getIpBasedContactInfoFromSparkConf(CassandraConnectorConf.scala:378)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnectorConf$.getContactInfoFromSparkConf(CassandraConnectorConf.scala:368)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnectorConf$.fromSparkConf(CassandraConnectorConf.scala:433)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnectorConf$.apply(CassandraConnectorConf.scala:358)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnector$.apply(CassandraConnector.scala:225)\n",
      "\tat org.apache.spark.sql.cassandra.DefaultSource.getTable(DefaultSource.scala:61)\n",
      "\tat org.apache.spark.sql.cassandra.DefaultSource.inferSchema(DefaultSource.scala:67)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:90)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.getTable$1(DataFrameWriter.scala:284)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:300)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy46.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "25/04/05 12:49:10 WARN ControlConnection: [s1] Error connecting to Node(endPoint=/127.0.0.1:9042, hostId=null, hashCode=39bf0fca), trying next node (ConnectionInitException: [s1|control|connecting...] Protocol initialization request, step 1 (OPTIONS): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/04/05 12:49:11 WARN ControlConnection: [s0] Error connecting to Node(endPoint=host.docker.internal/192.168.65.254:9042, hostId=9606303c-6b6b-48cb-a167-fbf34b3fae89, hashCode=11e0844a), trying next node (ConnectionInitException: [s0|control|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=fa3e9e08-1356-4644-b321-b221a1e0614d, APPLICATION_NAME=Spark-Cassandra-Connector-local-1743856642017}): failed to send request (java.nio.channels.NotYetConnectedException))\n",
      "25/04/05 12:49:12 ERROR MicroBatchExecution: Query [id = 883cd6a1-017e-459e-bbe6-1b9177b57798, runId = 4989273e-7e62-473c-b019-38b30e8a26b3] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py\", line 120, in call\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py\", line 117, in call\n",
      "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
      "  File \"/tmp/ipykernel_872/2343669047.py\", line 129, in <lambda>\n",
      "    .foreachBatch(lambda df_batch, batch_id:write_to_cassandra(df_batch, batch_id,cassandra_infos)) \\\n",
      "  File \"/tmp/ipykernel_872/2343669047.py\", line 75, in write_to_cassandra\n",
      "    df_batch.write \\\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/sql/readwriter.py\", line 1461, in save\n",
      "    self._jwrite.save()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o568.save.\n",
      ": java.io.IOException: Failed to open native connection to Cassandra at {} :: Could not reach any contact point, make sure you've provided valid addresses (showing first 1 nodes, use getAllErrors() for more): Node(endPoint=/127.0.0.1:9042, hostId=null, hashCode=39bf0fca): [com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s1|control|connecting...] Protocol initialization request, step 1 (OPTIONS): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException)]\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnector$.createSession(CassandraConnector.scala:173)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnector$.$anonfun$sessionCache$1(CassandraConnector.scala:161)\n",
      "\tat com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:32)\n",
      "\tat com.datastax.spark.connector.cql.RefCountedCache.syncAcquire(RefCountedCache.scala:69)\n",
      "\tat com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:57)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:81)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:103)\n",
      "\tat com.datastax.spark.connector.datasource.CassandraCatalog$.com$datastax$spark$connector$datasource$CassandraCatalog$$getMetadata(CassandraCatalog.scala:476)\n",
      "\tat com.datastax.spark.connector.datasource.CassandraCatalog$.getRelationMetaData(CassandraCatalog.scala:432)\n",
      "\tat org.apache.spark.sql.cassandra.DefaultSource.getTable(DefaultSource.scala:63)\n",
      "\tat org.apache.spark.sql.cassandra.DefaultSource.inferSchema(DefaultSource.scala:67)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:90)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.getTable$1(DataFrameWriter.scala:284)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:300)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy46.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: com.datastax.oss.driver.api.core.AllNodesFailedException: Could not reach any contact point, make sure you've provided valid addresses (showing first 1 nodes, use getAllErrors() for more): Node(endPoint=/127.0.0.1:9042, hostId=null, hashCode=39bf0fca): [com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s1|control|connecting...] Protocol initialization request, step 1 (OPTIONS): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException)]\n",
      "\tat com.datastax.oss.driver.api.core.AllNodesFailedException.copy(AllNodesFailedException.java:141)\n",
      "\tat com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures.getUninterruptibly(CompletableFutures.java:149)\n",
      "\tat com.datastax.oss.driver.api.core.session.SessionBuilder.build(SessionBuilder.java:835)\n",
      "\tat com.datastax.spark.connector.cql.DefaultConnectionFactory$.createSession(CassandraConnectionFactory.scala:143)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnector$.createSession(CassandraConnector.scala:167)\n",
      "\t... 58 more\n",
      "\tSuppressed: com.datastax.oss.driver.api.core.connection.ConnectionInitException: [s1|control|connecting...] Protocol initialization request, step 1 (OPTIONS): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException)\n",
      "\t\tat com.datastax.oss.driver.internal.core.channel.ProtocolInitHandler$InitRequest.fail(ProtocolInitHandler.java:356)\n",
      "\t\tat com.datastax.oss.driver.internal.core.channel.ChannelHandlerRequest.writeListener(ChannelHandlerRequest.java:87)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:184)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:30)\n",
      "\t\tat com.datastax.oss.driver.internal.core.channel.ChannelHandlerRequest.send(ChannelHandlerRequest.java:76)\n",
      "\t\tat com.datastax.oss.driver.internal.core.channel.ProtocolInitHandler$InitRequest.send(ProtocolInitHandler.java:193)\n",
      "\t\tat com.datastax.oss.driver.internal.core.channel.ProtocolInitHandler.onRealConnect(ProtocolInitHandler.java:124)\n",
      "\t\tat com.datastax.oss.driver.internal.core.channel.ConnectInitHandler.lambda$connect$0(ConnectInitHandler.java:57)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:571)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:550)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:321)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:337)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:707)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\t\tSuppressed: com.datastax.oss.driver.shaded.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /127.0.0.1:9042\n",
      "\t\tCaused by: java.net.ConnectException: Connection refused\n",
      "\t\t\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\t\t\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n",
      "\t\t\tat com.datastax.oss.driver.shaded.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n",
      "\t\t\tat com.datastax.oss.driver.shaded.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\t\t\tat com.datastax.oss.driver.shaded.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:707)\n",
      "\t\t\tat com.datastax.oss.driver.shaded.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n",
      "\t\t\tat com.datastax.oss.driver.shaded.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n",
      "\t\t\tat com.datastax.oss.driver.shaded.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\t\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\t\t\tat com.datastax.oss.driver.shaded.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\t\t\tat com.datastax.oss.driver.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t\t\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\tCaused by: com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException\n",
      "\t\tat com.datastax.oss.driver.shaded.netty.channel.AbstractChannel$AbstractUnsafe.flush0()(Unknown Source)\n",
      "\n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat com.sun.proxy.$Proxy46.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "25/04/05 12:50:03 WARN ControlConnection: [s0] Error connecting to Node(endPoint=host.docker.internal/192.168.65.254:9042, hostId=9606303c-6b6b-48cb-a167-fbf34b3fae89, hashCode=11e0844a), trying next node (ConnectionInitException: [s0|control|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=fa3e9e08-1356-4644-b321-b221a1e0614d, APPLICATION_NAME=Spark-Cassandra-Connector-local-1743856642017}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/04/05 12:50:04 WARN ChannelPool: [s0|host.docker.internal/192.168.65.254:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=fa3e9e08-1356-4644-b321-b221a1e0614d, APPLICATION_NAME=Spark-Cassandra-Connector-local-1743856642017}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/04/05 12:50:05 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from 020a52f2e8d8:43555 in 10000 milliseconds\n",
      "25/04/05 12:50:51 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:50:52 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:50:52 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "/tmp/ipykernel_872/2343669047.py:60: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "25/04/05 12:50:55 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:50:55 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:50:55 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:50:55 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:50:55 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:50:55 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "25/04/05 12:50:55 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "/tmp/ipykernel_872/2343669047.py:60: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n"
     ]
    }
   ],
   "source": [
    "fetched_fraud_prediction_schema=fetch_schema(\n",
    "    schema_regitry_url=shcema_registry,\n",
    "    topic=\"fraud_prediction\"\n",
    ")\n",
    "\n",
    "fetched_credit_data_schema=fetch_schema(\n",
    "    schema_regitry_url=\"http://host.docker.internal:8081\",\n",
    "    topic=\"raw_credit_data\"\n",
    ")\n",
    "\n",
    "kafka_infos_for_credit_data={\n",
    "    \"input\":{\n",
    "        \"broker\":\"host.docker.internal:9092\",\n",
    "        \"topic\":\"raw_credit_data\",\n",
    "        \"schema\":fetched_credit_data_schema\n",
    "    },\n",
    "    \"output\":{\n",
    "        \"broker\":\"host.docker.internal:9092\",\n",
    "        \"topic\":\"fraud_prediction\",\n",
    "        \"schema\":fetched_fraud_prediction_schema\n",
    "    },\n",
    "}\n",
    "\n",
    "cassandra_infos_for_credit_data={\n",
    "    \"keyspace\":\"fraud_detection\",\n",
    "    \"table\":\"credit_data\"\n",
    "}\n",
    "\n",
    "credit_fraud_kafka_stream_query,credit_fraud_cassandra_stream_query=process_stream(\n",
    "    cassandra_infos=cassandra_infos_for_credit_data,\n",
    "    kafka_infos=kafka_infos_for_credit_data,\n",
    "    pipeline=credit_data_pipline,\n",
    "    ml_model=credit_model,\n",
    "    pyspark_schema=raw_credit_data_schema\n",
    ")\n",
    "credit_fraud_kafka_stream_query.awaitTermination()\n",
    "credit_fraud_cassandra_stream_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6361b50a-7f46-4469-814a-f026392137a9",
   "metadata": {},
   "source": [
    "- Testing the process_stream function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd13906-7963-43d4-a0d6-ca234a33c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''fetched_fraud_prediction_schema=fetch_schema(\n",
    "    schema_regitry_url=shcema_registry,\n",
    "    topic=\"fraud_prediction\"\n",
    ")\n",
    "\n",
    "fetched_insurance_data_schema=fetch_schema(\n",
    "    schema_regitry_url=shcema_registry,\n",
    "    topic=insurance_input_topic\n",
    ")\n",
    "\n",
    "kafka_infos_for_credit_data={\n",
    "    \"input\":{\n",
    "        \"broker\":\"host.docker.internal:9092\",\n",
    "        \"topic\":\"raw_insurance_data\",\n",
    "        \"schema\":fetched_insurance_data_schema\n",
    "    },\n",
    "    \"output\":{\n",
    "        \"broker\":\"host.docker.internal:9092\",\n",
    "        \"topic\":\"fraud_prediction\",\n",
    "        \"schema\":fetched_fraud_prediction_schema\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "insurance_fraud_stream_query=process_stream(\n",
    "    kafka_infos=kafka_infos_for_credit_data,\n",
    "    pipeline=insurance_data_pipeline,\n",
    "    ml_model=insurance_model,\n",
    "    pyspark_schema=raw_insurance_data_schema\n",
    ")\n",
    "insurance_fraud_stream_query.awaitTermination()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35489289-9f52-4bd0-bbd4-d5cf36f8c6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
